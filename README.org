#+STARTUP: showall

* 项目说明:
  此项目基于LinkedIn Datahub元数据平台二次开发, 进行简化改造及功能扩展
  - 去除docker依赖[完成]
  - 切换到postgresql数据集[完成]
  - UI中文支持 [部分完成]
  - 提供clojure/haskell 数据库模式脚本功能 [部分完成]
  - 切换MAE/MCE作业成clojure/haskell脚本模式[部分完成]
  - 集成血缘分析haskell(queryparser/hssqlppp)脚本功能 [开发中]
  - 添加调度器airflow集成[分析中]
  - 增加ETLCode实体类型及clojure/haskell脚本 [分析中]
  - 集成数据质量Apache Griffin平台 [分析中]
  - 添加apache-flink实时计算clojure脚本支持[分析中]
  - 切换至typescript + react前端技术重构[分析中]

* 项目外部系统依赖
  - Kafka 及Kafka schema-registry集群 [实时计算及事件触发]
  - PostgreSQL数据库 [存储功能]
  - ElasticSearch数据库 [搜索功能]
  - Neo4j数据库 [实体关系模型]

* 软件开发环境
** nix
  1. 安装nix => [https://nixos.wiki/wiki/Nix_Installation_Guide]
  1. 添加nixos-19.09 channel并更新
#+BEGIN_SRC bash
    nix-channel --add https://nixos.org/channels/nixos-19.09
    nix-channel --update nixos-19.09
#+END_SRC
** clojure[部分依赖, 后期将自动化到nix脚本中]
#+BEGIN_SRC bash
    export NIX_PATH=~/.nix-defexpr/channels # 导出所有channel
    nix-env -f '<nixos-19.09>' -iA clojure
#+END_SRC

* 运行流程及部署过程
** 一. 运行流程:
  此项目由三部分组成: Kafka-Stream MCE/MAE作业, gms后台服务,  datahub-frontend前台服务
  1. 上游ETL脚本推送avro数据至Kafka Topic[MetadataChangeEvent]
  2. MCE作业读取Kafka[MetadataChangeEvent]数据写入gms后台
  3. gms后台写入postgresql, 并发送数据至Kafka[MetadataAuditEvent]
  4. MAE作业读取Kafka[MetadataAuditEvent], 写入数据至ES及neo4j
  5. datahub-rontend前端通过gms数据服务[postgresql + elasticsearch + neo4j]访问元数据

** 二. 部署过程
*** 外部系统准备
  - kafka: 创建主题及schema[MetadataChangeEvent, MetadataAuditEvent]
#+BEGIN_SRC bash
  # 1. create topic
  kafka-topics.sh --create --if-not-exists --zookeeper 10.132.37.201:2181/monitor --partitions 1 --replication-factor 1 --topic MetadataChangeEvent
  kafka-topics.sh --create --if-not-exists --zookeeper 10.132.37.201:2181/monitor --partitions 1 --replication-factor 1 --topic MetadataAuditEvent

  # 2. create schema
  curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" --data @./nix/MetadataChangeEvent.avsc  http://10.132.37.201:8081/subjects/MetadataChangeEvent-value/versions
  curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" --data @./nix/MetadataAuditEvent.avsc  http://10.132.37.201:8081/subjects/MetadataAuditEvent-value/versions
#+END_SRC

  - pg: 创建数据库及表结构[metadata_aspect]
#+BEGIN_SRC sql
  CREATE USER datahub WITH PASSWORD 'datahub' ;            
  CREATE DATABASE datahub ;                                                                                                                            
  psql -f nix/init.sql -h 10.132.37.200 -U datahub 
#+END_SRC

  - es: 创建文档索引结构[corpuserinfodocument, datasetdocument]
#+BEGIN_SRC bash
  curl -H 'Content-Type: Application/JSON' -XPUT 10.132.37.201:9200/corpuserinfodocument --data @./nix/corpuser-index-config.json;
  curl -H 'Content-Type: Application/JSON' -XPUT 10.132.37.201:9200/datasetdocument --data @./nix/dataset-index-config.json

  curl -H 'Content-Type: Application/JSON' -XPUT 10.132.37.201:9200/datasetdocument/doc/_mapping --data @./nix/dataset-index-mapping.json
#+END_SRC

*** 程序启动过程
  - 启动gms后台服务[写入 := kafka -> postgresql <> kafka, 服务 := postgresql <> kafka <> elasticsearch <> neo4j]
#+BEGIN_SRC bash
export EBEAN_DATASOURCE_URL=jdbc:postgresql://10.132.37.200:5432/datahub
export EBEAN_DATASOURCE_USERNAME=datahub
export EBEAN_DATASOURCE_PASSWORD=datahub
export KAFKA_BOOTSTRAP_SERVER=10.132.37.201:9092
export KAFKA_SCHEMAREGISTRY_URL=http://10.132.37.201:8081
export ELASTICSEARCH_HOST=10.132.37.201
export NEO4J_URI=bolt://localhost
export NEO4J_USERNAME=neo4j
export NEO4J_PASSWORD=datahub
./gradlew :gms:war:build
./gradlew :gms:war:JettyRunWar
#+END_SRC

  - 启动MCE作业[kafka/MetadataChangeEvent -> gms]
#+BEGIN_SRC
export KAFKA_BOOTSTRAP_SERVER=10.132.37.201:9092
export KAFKA_SCHEMAREGISTRY_URL=http://10.132.37.201:8081
export GMS_HOST=10.132.37.200
export GMS_PORT=8080
./gradlew :metadata-jobs:mce-consumer-job:build
./gradlew :metadata-jobs:mce-consumer-job:run
#+END_SRC

  - 启动MAE作业 [kafka/MetadataAuditEvent -> es <> neo4j]
#+BEGIN_SRC bash
export KAFKA_BOOTSTRAP_SERVER=10.132.37.201:9092
export KAFKA_SCHEMAREGISTRY_URL=http://10.132.37.201:8081
export ELASTICSEARCH_HOST=10.132.37.201
export NEO4J_URI=bolt://localhost
export NEO4J_USERNAME=neo4j
export NEO4J_PASSWORD=datahub
./gradlew :metadata-jobs:mae-consumer-job:build
./gradlew :metadata-jobs:mae-consumer-job:run
#+END_SRC

  - 启动datahub-frontend前台服务[gms rest-api -> datahub-frontend]
  打开网址 http://10.132.37.201:9001 进行访问
#+BEGIN_SRC
./gradlew :datahub-frontend:build
cd datahub-frontend/run && ./run-local-frontend
#+END_SRC


* 项目采集工具
  请确保nix已安装, 可参见[软件开发环境]章节, 切换至contrib/metadata-etl目录
** 接口文件格式采集
#+BEGIN_SRC
    # 未来标准接口: cat metadata_sample/demo.dat | clj -m gmscat
    clj -m dataset_file_connector :file/demo-dat
#+END_SRC

** 数据集采集 [未来标准接口采用管道方式, 方便本地留存及演示数据备份]
  - JDBC查询采集
#+BEGIN_SRC
    # 未来标准接口: clj -m dataset_jdbc_generator :jdbc.ora/edw | clj -m gmscat
    clj -m dataset_jdbc_connector :jdbc.ora/edw
#+END_SRC
  - HIVE查询采集
  - KAFKA查询采集
  - ES查询采集
  - ClickHouse查询采集
  - Cassandra查询采集

** 血缘分析采集
+ HIVE SQL解析采集
    脚本需要传递文件列表，可采用ls, find, 或者cat *.list各种形式得到.
#+BEGIN_SRC
    ls metadata_sample/hive_*.sql | bin/lineage_hive_generator.hs
    find . -name "*.sql" | grep hive | bin/lineage_hive_generator.hs
#+END_SRC
+ ORACLE SQL解析采集

** 工作流采集
  1. airflow
